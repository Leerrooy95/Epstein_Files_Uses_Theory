# ⚠️ Transparency Notice

**This folder is maintained by GitHub Copilot (Claude, Opus 4.6), an AI language
model.  Every document here was generated by an AI, not a human researcher.**

AI models can be wrong.  They can hallucinate sources, misinterpret data, and
introduce biases the operator never intended.  Treat everything in this folder
as *analysis to be verified*, not as established fact.

---

# Copilot Opus 4.6 — Lead Researcher Analysis

**Analyst:** GitHub Copilot (Claude, Opus 4.6)  
**Role:** Lead Researcher — verification, methodology review, and recommendations  
**Created:** February 8, 2026  
**Repository owner:** Austin Smith ([@Leerrooy95](https://github.com/Leerrooy95))

---

## How This Folder Works

The repository owner builds datasets and runs correlations.  My job is to:

1. **Verify** — independently check whether reported correlations hold up
   under scrutiny (permutation tests, robustness checks, alternative methods)
2. **Recommend** — suggest improvements to datasets (coverage gaps, category
   standardization, normalization) and flag issues that could weaken findings
3. **Expand** — propose new correlation techniques and analytical approaches
   the owner may not have encountered yet (Spearman, Granger causality,
   rolling windows, event-study designs, etc.)

I do **not** build datasets or run the initial correlations — that's the
owner's work.  I receive the data and results, then analyze them here.

---

## Folder Structure

```
Copilot_Opus_4.6_Analysis/
├── README.md                 ← You are here
├── Statistical_Tests/        ← Runnable Python scripts
│   ├── original_data_loader.py   ← Shared module: loads original pre-2026 datasets
│   ├── permutation_test.py   ← Shuffle-based significance testing
│   ├── year_distribution_audit.py  ← Year-skew diagnosis across all CSVs
│   ├── autocorrelation_adjusted_test.py  ← Durbin-Watson + block bootstrap
│   ├── normalized_correlation.py  ← Per-year equalized event-count correlation
│   ├── cross_validation_dec2025.py  ← Does the pattern hold without Dec 2025?
│   ├── rolling_window_correlation.py  ← Sliding-window correlation over time
│   ├── event_study_framework.py  ← Compliance response after friction events
│   └── granger_causality_test.py  ← Predictive direction test (friction→compliance vs reverse)
├── Findings/                 ← Written analysis and documentation
│   ├── dataset_provenance.md ← Which data feeds which correlation (git-traced)
│   ├── backfill_guide.md     ← Recommendations for evening out year coverage
│   ├── correlation_summary.md ← All five reported correlations in one place
│   ├── new_analysis_findings.md ← Results of all five robustness tests
│   └── granger_causality_results.md ← Granger causality test findings
├── Verification_Reports/     ← Scrutiny and verification audits
│   ├── scrutiny_report_feb8_2026.md ← Full scrutiny of all prior work
│   └── prediction_tracker_feb9_2026.md ← Testable predictions vs real-world outcomes
├── Datasets/                 ← Local copies of ORIGINAL pre-2026 CSVs
│   ├── master_reflexive_correlation_data.csv  (Control_Proof — 30 weeks)
│   ├── reflexive_control_scraped_data.csv     (Control_Proof — OSINT triggers)
│   ├── MASTER_reflexive_control_v2.csv        (Control_Proof — 149 events)
│   ├── thermostat_control_data.csv            (Control_Proof — 150 events)
│   ├── project_trident_final_dossier.csv      (Project_Trident — 118 entries)
│   ├── anchor_events_parsed.csv               (Project_Trident — 70 anchors)
│   ├── ritual_events_parsed.csv               (Project_Trident — 51 rituals)
│   ├── temporal_correlations_analyzed.csv      (Project_Trident — 338 pairings)
│   ├── Lag_Correlation_Analysis_Verified_Holidays.csv  (Best_Data — 533 records)
│   ├── Expanded_Policy_Anchors.csv            (Best_Data — 11 anchors)
│   ├── Holidays_2015_2025_Verified.csv        (Best_Data — 59 holidays)
│   ├── aid_timeline_clean.csv                 (Best_Data — 190 aid events)
│   ├── policy_cleaned.csv                     (Best_Data — 60 policies)
│   ├── tech_filled_dates.csv                  (Best_Data — 357 tech events)
│   ├── Fund_Flow_Ritual_Coordination_2025.csv (Best_Data — 97 fund flows)
│   ├── Coalition_Narrative_Map_2015-2025.csv  (Silicon_Sovereignty — 456 events)
│   ├── VOCA_funding_timeline_clean.csv        (Silicon_Sovereignty — 667 events)
│   ├── Regulatory_Map_Data_CLEANED.csv        (Silicon_Sovereignty — 76 events)
│   └── REFINED_supercomputer_geopolitics (1).csv  (Silicon_Sovereignty — 906 events)
└── Consolidation_Analysis/   ← Cross-domain consolidation pattern assessment
    └── consolidation_pattern_significance.md ← Infrastructure consolidation significance report (Feb 9, 2026)
```

### `Statistical_Tests/`

Runnable Python scripts that test claims in the data.  All multi-dataset
scripts import from `original_data_loader.py` which provides standardized
access to the original pre-2026 datasets.  Anyone can execute these:

```bash
pip install pandas numpy scipy statsmodels
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/permutation_test.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/year_distribution_audit.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/autocorrelation_adjusted_test.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/normalized_correlation.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/cross_validation_dec2025.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/rolling_window_correlation.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/event_study_framework.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/granger_causality_test.py
```

### `Datasets/`

Local copies of the **original pre-2026 CSV files** used by the statistical
tests.  These are the exact datasets Austin used for his original correlations
(v3.1–v5.5, December 22–25, 2025).  Originals remain untouched in their
original locations (`Control_Proof/`, `Project_Trident/`,
`Project_Trident/Best_Data_For_Project_Trident/`, and `09_Silicon_Sovereignty/`).

**Important:** New_Data_2026/ files (BlackRock, Biopharma, Infrastructure, etc.)
are explicitly EXCLUDED — they were uploaded January 3, 2026 and were not part
of the original correlations.  All files verified identical to their original
git commits via `diff` comparison.

### `Findings/`

Written analysis documents — provenance traces, methodology reviews,
dataset improvement recommendations, verification reports, and Granger
causality results.

### `Verification_Reports/`

Scrutiny audits verifying the legitimacy and reproducibility of all prior
work in this folder.  Includes script re-execution results, factual claim
checks against authoritative sources, methodology review, and prediction
tracking against real-world outcomes.

### `Consolidation_Analysis/`

Cross-domain analysis assessing whether the Infrastructure Consolidation
Pattern documented elsewhere in the repository is statistically and
structurally significant.  Evaluates consortium overlap, temporal clustering,
state-level regulatory enablement (Arkansas), and geopolitical complicating
factors (Saudi-UAE rupture, Venezuela compliance window).  All major claims
independently verified via web search.

---

## Work Completed

| Date | File | Type | Summary |
|------|------|------|---------|
| Feb 8 | `Statistical_Tests/permutation_test.py` | Verification | 1K-shuffle test on 30-row master CSV + 10K-shuffle multi-dataset test. CORE Pearson r=0.62 significant (p<0.0001) but Spearman ρ=0.02 not significant — correlation driven by magnitude outliers. |
| Feb 8 | `Statistical_Tests/year_distribution_audit.py` | Verification | 2025 has 4.7× the 2015–2024 average. ~56% scraping artifact, ~44% genuine coverage spike. |
| Feb 8 | `Findings/dataset_provenance.md` | Verification | Original r=0.6196 (Dec 23, 2025) used only 30-row master CSV. New_Data_2026 uploaded Jan 4 — 12 days later. Two separate analyses. |
| Feb 8 | `Findings/backfill_guide.md` | Recommendation | Search queries and verified anchor events for backfilling 4 most skewed CSVs. Target distributions benchmarked against BlackRock reference. |
| Feb 8 | `Findings/correlation_summary.md` | Reference | Consolidated reference for all five reported correlations (original r=0.6196, Project Trident p=0.002, cross-validation χ²=330.62, updated r=0.6685, full-scope r=0.5268) — which datasets feed which number, reproduction scripts, and known limitations. |
| Feb 8 | `Statistical_Tests/autocorrelation_adjusted_test.py` | Verification | Durbin-Watson (DW=1.29 core, 1.82 full) + block bootstrap (4-week blocks, 10K iterations). Pearson survives adjustment (p=0.0004 core, 0.0001 full). Core Spearman fails (p=0.78); full Spearman survives (p=0.007). |
| Feb 8 | `Statistical_Tests/normalized_correlation.py` | Verification | Three normalization methods: z-score reduces r from 0.62→0.23 (core) and 0.53→0.29 (full); proportional scaling → near zero; binary → negative. The raw r is inflated by 2025 magnitude. |
| Feb 8 | `Statistical_Tests/cross_validation_dec2025.py` | Verification | Removing Dec 2025: core r drops 90% (0.62→0.06, not significant). Full r drops 82% (0.53→0.09, not significant). The entire correlation is driven by one month. |
| Feb 8 | `Statistical_Tests/rolling_window_correlation.py` | Verification | 26-week rolling mean r=0.20 (core), 0.19 (full). Only 11–28% of windows exceed r>0.3. Strong windows concentrated in late 2025. Weak but consistently positive signal across earlier periods. |
| Feb 8 | `Statistical_Tests/event_study_framework.py` | Verification | Post-friction windows have FEWER compliance events than pre-friction (ratio 0.56x at 14d). However, friction dates attract 3.5x more compliance than random dates — colocation, not causation. |
| Feb 8 | `Findings/new_analysis_findings.md` | Reference | Comprehensive synthesis of all five robustness tests with executive summary, interpretation, and recommendations. |
| Feb 8 | `Statistical_Tests/granger_causality_test.py` | Verification | Granger causality at lags 1-8. Original 30-row: friction→compliance at lags 1-2 ✓. Core scope: compliance→friction only (REVERSE). Full scope: bidirectional — suggests common driver. |
| Feb 8 | `Findings/granger_causality_results.md` | Reference | Full write-up of Granger findings with interpretation, limitations, and connection to event-study results. |
| Feb 8 | `Verification_Reports/scrutiny_report_feb8_2026.md` | Verification | Complete scrutiny of all prior work: all 7 scripts re-executed and reproduced, all factual claims verified against authoritative sources, methodology reviewed for correctness. |
| Feb 8 | `Datasets/*` + all `Statistical_Tests/*.py` | **CRITICAL FIX** | **Replaced all New_Data_2026 files with original pre-2026 datasets.** Previous analysis incorrectly used BlackRock, Biopharma, Infrastructure, etc. (uploaded Jan 3, 2026) instead of Austin's original datasets from Control_Proof, Project_Trident, and 09_Silicon_Sovereignty (Dec 22-25, 2025). Created `original_data_loader.py` shared module. All 8 scripts updated and verified. |

| Feb 9 | `Consolidation_Analysis/consolidation_pattern_significance.md` | Analysis | Infrastructure consolidation pattern significance assessment. Cross-references corrected correlations, Arkansas audit, geopolitical vectors, and Claude Code Analysis. All major claims web-verified. |
| Feb 9 | `Verification_Reports/prediction_tracker_feb9_2026.md` | Verification | Tracked all 8 README testable predictions against real-world outcomes. 2 confirmed, 1 date discrepancy (Tu BiShvat), 5 pending. Mandelson outcome exceeded prediction. All claims sourced to public reporting. |

### Planned work

- [x] Autocorrelation-adjusted significance test (Durbin-Watson / block bootstrap)
- [x] Normalized event-count correlation (events per year equalized)
- [x] Cross-validation: does the pattern hold when Dec 2025 is excluded?
- [x] Rolling-window correlation analysis (does r change across different time periods?)
- [x] Event-study framework (measure compliance response in a defined window around each friction event)
- [x] Granger causality test (does past friction predict future compliance, or vice versa?)
- [x] Scrutiny audit of all prior work (re-execute scripts, verify factual claims, review methodology)
- [x] **Dataset correction: replace New_Data_2026 files with original pre-2026 datasets**

### Future recommendations

- [ ] Re-run Granger causality excluding December 2025 to test robustness
- [ ] Test with first-differenced series (Δfriction, Δcompliance) to address stationarity
- [ ] Investigate the hand-scored vs event-count discrepancy in Granger direction
- [ ] Partial correlation controlling for a "political activity" index (e.g., congressional session calendar)
- [ ] Backfill earlier years per backfill_guide.md to enable fairer cross-year comparisons

---

## Methodology Standards

### 1. No narrative bias

This project sits at the intersection of politics, finance, and intelligence —
areas where confirmation bias is the default failure mode.  My job is to test
whether patterns in the data are statistically real, not to argue that they
prove any particular theory.

**What I will do:**
- Report results that weaken the thesis with the same prominence as results
  that strengthen it
- Flag when a finding is driven by outliers or sample composition
- Distinguish between "statistically significant" and "practically meaningful"

**What I will not do:**
- Cherry-pick time windows or subsets to make correlations look stronger
- Use loaded language ("proves," "confirms," "exposes") for statistical results
- Treat correlation as causation

### 2. Recommendations and new techniques

When reviewing datasets and correlations, I will actively suggest:

- **Dataset improvements** — missing year coverage, inconsistent categories,
  date format issues, duplicate records, normalization needs
- **Alternative statistical methods** — when a standard Pearson r may not be
  the right tool (e.g., Spearman for non-linear relationships, Granger for
  temporal causality, rolling correlations for regime changes, partial
  correlations to control for confounders)
- **Robustness checks** — ways to stress-test a finding (leave-one-out,
  bootstrap confidence intervals, sensitivity to outlier removal)
- **Visualization approaches** — plots that reveal patterns or problems
  the raw numbers might hide

I will explain *why* I'm suggesting each technique in plain language so the
owner can learn the methodology, not just receive a number.

### 3. Factual accuracy

- Every historical event I reference must be verifiable via the cited source
- If I use web search results, I will include the URL
- If I cannot verify a claim, I will say so explicitly
- I will not backfill datasets with AI-generated events — the user must
  verify every entry against a real source

### 4. Transparency about limitations

**What I am:**
- An AI language model (Claude, Opus 4.6) running as GitHub Copilot
- Good at: pattern recognition, statistical computation, code generation,
  synthesizing large amounts of text
- Capable of web search for fact-checking when the tool is available

**What I am not:**
- An investigative journalist with domain expertise
- A statistician with peer-reviewed methodology
- Infallible — I make mistakes, especially with dates, names, and numbers

**Known limitations of my analysis so far:**

| Finding | Limitation |
|---------|-----------|
| Permutation test passes (p < 0.001) for 30-row master | Tests only whether correlation ≠ 0, not whether the mechanism claim is correct |
| Multi-dataset Pearson r ≈ 0.30 (original datasets) | Lower than the 30-row hand-scored r = 0.62, reflecting the difference between curated indices and raw event counts |
| Spearman ρ ≈ 0.37 (original datasets) | Stronger than Pearson — rank correlation is robust to outlier magnitudes |
| 2025 skew diagnosis | Based on date parsing which may miss events with non-standard date formats |
| Dataset provenance | Git history only shows when files were *pushed to GitHub*, not when analysis was *first run locally* |
| Autocorrelation adjustment survives (p=0.0002) | Durbin-Watson DW ≈ 1.70 indicates mild autocorrelation; block bootstrap still shows significance |
| Dec 2025 exclusion: correlation SURVIVES (was: destroyed) | With original datasets, removing Dec 2025 only reduces r by ~29% (0.30 → 0.22), not 90% — the signal is distributed across the timeline |
| Event-study: colocation not causation | Friction dates attract ~50x more compliance than random dates, but post-friction ≈ pre-friction |
| Normalized r: z-score drops to ~0.04 (marginal) | Per-year normalization removes most of the signal; binary presence/absence r ≈ 0.36 is strong |
| Rolling-window mean r ≈ 0.05 (13-week) | Many windows have constant series due to sparse events, limiting valid comparisons |
| Granger causality: bidirectional | Both directions significant at all lags — suggests common driver, not sequential mechanism |
| Granger (30-row master): friction → compliance lags 1-2 | Supports thesis in hand-scored data |

### 5. Political neutrality

The datasets in this repository touch on US domestic politics, Middle East
geopolitics, intelligence operations, and financial regulation.  These are
topics where people hold strong views.

My approach:
- I analyze data patterns, not political intentions
- I do not assign moral valence to correlations
- I treat all political actors symmetrically in my analysis
- If the data shows something, I report it; if it doesn't, I say so
- I will not speculate about motivations, conspiracies, or cover-ups
  beyond what the statistical evidence supports

### 6. Folder hygiene

- Each document in this folder will state its purpose, date, and data sources
  in the header
- Superseded documents will be marked as such, not silently deleted
- This README will be kept current as new analyses are added
- Subfolders will be added as needed to keep the structure navigable

---

## How to evaluate this work

If you're reading this and want to check whether my analysis is sound:

1. **Run the scripts yourself** — everything in `Statistical_Tests/` is
   executable Python.  `pip install pandas numpy scipy` and run them.
2. **Check the data** — all CSVs are in the repo.  Verify event dates and
   categories against the cited source URLs.
3. **Challenge the methodology** — if you think a statistical test is wrong
   or a conclusion doesn't follow, open an issue.
4. **Look for what I missed** — my biggest blind spot is things I didn't
   think to test.

---

*This document was written by GitHub Copilot (Claude, Opus 4.6) on February 8, 2026.*  
*Last updated: February 9, 2026 — Added prediction tracker (Verification_Reports/prediction_tracker_feb9_2026.md) verifying README testable predictions against real-world outcomes.*
