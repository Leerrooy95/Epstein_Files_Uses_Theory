# ⚠️ Transparency Notice

**This folder is maintained by GitHub Copilot (Claude, Opus 4.6), an AI language
model.  Every document here was generated by an AI, not a human researcher.**

AI models can be wrong.  They can hallucinate sources, misinterpret data, and
introduce biases the operator never intended.  Treat everything in this folder
as *analysis to be verified*, not as established fact.

---

# Copilot Opus 4.6 — Lead Researcher Analysis

**Analyst:** GitHub Copilot (Claude, Opus 4.6)  
**Role:** Lead Researcher, Project Trident statistical audit  
**Created:** February 8, 2026  
**Repository owner:** Austin Smith ([@Leerrooy95](https://github.com/Leerrooy95))

---

## What This Folder Contains

This is the working directory for statistical and methodological analysis
conducted by an AI assistant on the Regulated Friction Project datasets.
The focus is on testing claims, not advocating for them.

### Work completed so far

| File | Location | What it does |
|------|----------|-------------|
| `permutation_test.py` | `Run_Correlations_Yourself/` | 10,000-shuffle significance test on both the 30-row master CSV and the multi-dataset event counts. Reports Pearson r and Spearman ρ. |
| `year_distribution_audit.py` | `Run_Correlations_Yourself/` | Counts events by year across every CSV in the repo. Diagnoses the 2025 over-representation as ~56% scraping artifact, ~44% genuine coverage spike. |
| `dataset_provenance.md` | `Run_Correlations_Yourself/` | Git-commit-level trace showing the original r=0.6196 (Dec 23, 2025) used only the 30-row master CSV; the New_Data_2026 files were uploaded 12 days later. |
| `backfill_guide.md` | `Run_Correlations_Yourself/` | Search queries and verified anchor events for evening out the 2025-skewed datasets. No fabricated data — user must verify sources. |

### Planned work

- [ ] Autocorrelation-adjusted significance test (Durbin-Watson / block bootstrap)
- [ ] Normalized event-count correlation (events per year equalized)
- [ ] Cross-validation: does the pattern hold when Dec 2025 is excluded?

---

## Methodology Standards

### 1. No narrative bias

This project sits at the intersection of politics, finance, and intelligence —
areas where confirmation bias is the default failure mode.  My job is to test
whether patterns in the data are statistically real, not to argue that they
prove any particular theory.

**What I will do:**
- Report results that weaken the thesis with the same prominence as results
  that strengthen it
- Flag when a finding is driven by outliers or sample composition
- Distinguish between "statistically significant" and "practically meaningful"

**What I will not do:**
- Cherry-pick time windows or subsets to make correlations look stronger
- Use loaded language ("proves," "confirms," "exposes") for statistical results
- Treat correlation as causation

### 2. Factual accuracy

- Every historical event I reference must be verifiable via the cited source
- If I use web search results, I will include the URL
- If I cannot verify a claim, I will say so explicitly
- I will not backfill datasets with AI-generated events — the user must
  verify every entry against a real source

### 3. Transparency about limitations

**What I am:**
- An AI language model (Claude, Opus 4.6) running as GitHub Copilot
- Good at: pattern recognition, statistical computation, code generation,
  synthesizing large amounts of text
- Capable of web search for fact-checking when the tool is available

**What I am not:**
- An investigative journalist with domain expertise
- A statistician with peer-reviewed methodology
- Infallible — I make mistakes, especially with dates, names, and numbers

**Known limitations of my analysis so far:**

| Finding | Limitation |
|---------|-----------|
| Permutation test passes (p < 0.001) | Tests only whether correlation ≠ 0, not whether the mechanism claim is correct |
| Spearman ρ near zero for core scope | Suggests Pearson r is driven by magnitude outliers, not rank consistency |
| 2025 skew diagnosis | Based on date parsing which may miss events with non-standard date formats |
| Dataset provenance | Git history only shows when files were *pushed to GitHub*, not when analysis was *first run locally* |

### 4. Political neutrality

The datasets in this repository touch on US domestic politics, Middle East
geopolitics, intelligence operations, and financial regulation.  These are
topics where people hold strong views.

My approach:
- I analyze data patterns, not political intentions
- I do not assign moral valence to correlations
- I treat all political actors symmetrically in my analysis
- If the data shows something, I report it; if it doesn't, I say so
- I will not speculate about motivations, conspiracies, or cover-ups
  beyond what the statistical evidence supports

### 5. Folder hygiene

- Each document in this folder will state its purpose, date, and data sources
  in the header
- Superseded documents will be marked as such, not silently deleted
- The README (this file) will be kept current as new analyses are added

---

## How to evaluate this work

If you're reading this and want to check whether my analysis is sound:

1. **Run the scripts yourself** — everything in `Run_Correlations_Yourself/`
   is executable Python.  `pip install pandas numpy scipy` and run them.
2. **Check the data** — all CSVs are in the repo.  Verify event dates and
   categories against the cited source URLs.
3. **Challenge the methodology** — if you think a statistical test is wrong
   or a conclusion doesn't follow, open an issue.
4. **Look for what I missed** — my biggest blind spot is things I didn't
   think to test.

---

*This document was written by GitHub Copilot (Claude, Opus 4.6) on February 8, 2026.*
*It will be updated as new analyses are added to this folder.*
