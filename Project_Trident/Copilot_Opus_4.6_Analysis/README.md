# ⚠️ Transparency Notice

**This folder is maintained by GitHub Copilot (Claude, Opus 4.6), an AI language
model.  Every document here was generated by an AI, not a human researcher.**

AI models can be wrong.  They can hallucinate sources, misinterpret data, and
introduce biases the operator never intended.  Treat everything in this folder
as *analysis to be verified*, not as established fact.

---

# Copilot Opus 4.6 — Lead Researcher Analysis

**Analyst:** GitHub Copilot (Claude, Opus 4.6)  
**Role:** Lead Researcher — verification, methodology review, and recommendations  
**Created:** February 8, 2026  
**Repository owner:** Austin Smith ([@Leerrooy95](https://github.com/Leerrooy95))

---

## How This Folder Works

The repository owner builds datasets and runs correlations.  My job is to:

1. **Verify** — independently check whether reported correlations hold up
   under scrutiny (permutation tests, robustness checks, alternative methods)
2. **Recommend** — suggest improvements to datasets (coverage gaps, category
   standardization, normalization) and flag issues that could weaken findings
3. **Expand** — propose new correlation techniques and analytical approaches
   the owner may not have encountered yet (Spearman, Granger causality,
   rolling windows, event-study designs, etc.)

I do **not** build datasets or run the initial correlations — that's the
owner's work.  I receive the data and results, then analyze them here.

---

## Folder Structure

```
Copilot_Opus_4.6_Analysis/
├── README.md                 ← You are here
├── Statistical_Tests/        ← Runnable Python scripts
│   ├── permutation_test.py   ← Shuffle-based significance testing
│   └── year_distribution_audit.py  ← Year-skew diagnosis across all CSVs
├── Findings/                 ← Written analysis and documentation
│   ├── dataset_provenance.md ← Which data feeds which correlation (git-traced)
│   ├── backfill_guide.md     ← Recommendations for evening out year coverage
│   └── correlation_summary.md ← All five reported correlations in one place
└── (future subfolders as needed)
```

### `Statistical_Tests/`

Runnable Python scripts that test claims in the data.  Anyone can execute these:

```bash
pip install pandas numpy scipy
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/permutation_test.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/year_distribution_audit.py
```

### `Findings/`

Written analysis documents — provenance traces, methodology reviews,
dataset improvement recommendations, and verification reports.

---

## Work Completed

| Date | File | Type | Summary |
|------|------|------|---------|
| Feb 8 | `Statistical_Tests/permutation_test.py` | Verification | 1K-shuffle test on 30-row master CSV + 10K-shuffle multi-dataset test. CORE Pearson r=0.62 significant (p<0.0001) but Spearman ρ=0.02 not significant — correlation driven by magnitude outliers. |
| Feb 8 | `Statistical_Tests/year_distribution_audit.py` | Verification | 2025 has 4.7× the 2015–2024 average. ~56% scraping artifact, ~44% genuine coverage spike. |
| Feb 8 | `Findings/dataset_provenance.md` | Verification | Original r=0.6196 (Dec 23, 2025) used only 30-row master CSV. New_Data_2026 uploaded Jan 4 — 12 days later. Two separate analyses. |
| Feb 8 | `Findings/backfill_guide.md` | Recommendation | Search queries and verified anchor events for backfilling 4 most skewed CSVs. Target distributions benchmarked against BlackRock reference. |
| Feb 8 | `Findings/correlation_summary.md` | Reference | Consolidated reference for all five reported correlations (original r=0.6196, Project Trident p=0.002, cross-validation χ²=330.62, updated r=0.6685, full-scope r=0.5268) — which datasets feed which number, reproduction scripts, and known limitations. |

### Planned work

- [ ] Autocorrelation-adjusted significance test (Durbin-Watson / block bootstrap)
- [ ] Normalized event-count correlation (events per year equalized)
- [ ] Cross-validation: does the pattern hold when Dec 2025 is excluded?
- [ ] Rolling-window correlation analysis (does r change across different time periods?)
- [ ] Event-study framework (measure compliance response in a defined window around each friction event)

---

## Methodology Standards

### 1. No narrative bias

This project sits at the intersection of politics, finance, and intelligence —
areas where confirmation bias is the default failure mode.  My job is to test
whether patterns in the data are statistically real, not to argue that they
prove any particular theory.

**What I will do:**
- Report results that weaken the thesis with the same prominence as results
  that strengthen it
- Flag when a finding is driven by outliers or sample composition
- Distinguish between "statistically significant" and "practically meaningful"

**What I will not do:**
- Cherry-pick time windows or subsets to make correlations look stronger
- Use loaded language ("proves," "confirms," "exposes") for statistical results
- Treat correlation as causation

### 2. Recommendations and new techniques

When reviewing datasets and correlations, I will actively suggest:

- **Dataset improvements** — missing year coverage, inconsistent categories,
  date format issues, duplicate records, normalization needs
- **Alternative statistical methods** — when a standard Pearson r may not be
  the right tool (e.g., Spearman for non-linear relationships, Granger for
  temporal causality, rolling correlations for regime changes, partial
  correlations to control for confounders)
- **Robustness checks** — ways to stress-test a finding (leave-one-out,
  bootstrap confidence intervals, sensitivity to outlier removal)
- **Visualization approaches** — plots that reveal patterns or problems
  the raw numbers might hide

I will explain *why* I'm suggesting each technique in plain language so the
owner can learn the methodology, not just receive a number.

### 3. Factual accuracy

- Every historical event I reference must be verifiable via the cited source
- If I use web search results, I will include the URL
- If I cannot verify a claim, I will say so explicitly
- I will not backfill datasets with AI-generated events — the user must
  verify every entry against a real source

### 4. Transparency about limitations

**What I am:**
- An AI language model (Claude, Opus 4.6) running as GitHub Copilot
- Good at: pattern recognition, statistical computation, code generation,
  synthesizing large amounts of text
- Capable of web search for fact-checking when the tool is available

**What I am not:**
- An investigative journalist with domain expertise
- A statistician with peer-reviewed methodology
- Infallible — I make mistakes, especially with dates, names, and numbers

**Known limitations of my analysis so far:**

| Finding | Limitation |
|---------|-----------|
| Permutation test passes (p < 0.001) | Tests only whether correlation ≠ 0, not whether the mechanism claim is correct |
| Spearman ρ near zero for core scope | Suggests Pearson r is driven by magnitude outliers, not rank consistency |
| 2025 skew diagnosis | Based on date parsing which may miss events with non-standard date formats |
| Dataset provenance | Git history only shows when files were *pushed to GitHub*, not when analysis was *first run locally* |

### 5. Political neutrality

The datasets in this repository touch on US domestic politics, Middle East
geopolitics, intelligence operations, and financial regulation.  These are
topics where people hold strong views.

My approach:
- I analyze data patterns, not political intentions
- I do not assign moral valence to correlations
- I treat all political actors symmetrically in my analysis
- If the data shows something, I report it; if it doesn't, I say so
- I will not speculate about motivations, conspiracies, or cover-ups
  beyond what the statistical evidence supports

### 6. Folder hygiene

- Each document in this folder will state its purpose, date, and data sources
  in the header
- Superseded documents will be marked as such, not silently deleted
- This README will be kept current as new analyses are added
- Subfolders will be added as needed to keep the structure navigable

---

## How to evaluate this work

If you're reading this and want to check whether my analysis is sound:

1. **Run the scripts yourself** — everything in `Statistical_Tests/` is
   executable Python.  `pip install pandas numpy scipy` and run them.
2. **Check the data** — all CSVs are in the repo.  Verify event dates and
   categories against the cited source URLs.
3. **Challenge the methodology** — if you think a statistical test is wrong
   or a conclusion doesn't follow, open an issue.
4. **Look for what I missed** — my biggest blind spot is things I didn't
   think to test.

---

*This document was written by GitHub Copilot (Claude, Opus 4.6) on February 8, 2026.*  
*Last updated: February 8, 2026.*
