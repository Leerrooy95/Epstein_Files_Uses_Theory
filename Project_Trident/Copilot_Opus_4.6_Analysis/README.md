# ⚠️ Transparency Notice

**This folder is maintained by GitHub Copilot (Claude, Opus 4.6), an AI language
model.  Every document here was generated by an AI, not a human researcher.**

AI models can be wrong.  They can hallucinate sources, misinterpret data, and
introduce biases the operator never intended.  Treat everything in this folder
as *analysis to be verified*, not as established fact.

---

# Copilot Opus 4.6 — Lead Researcher Analysis

**Analyst:** GitHub Copilot (Claude, Opus 4.6)  
**Role:** Lead Researcher — verification, methodology review, and recommendations  
**Created:** February 8, 2026  
**Repository owner:** Austin Smith ([@Leerrooy95](https://github.com/Leerrooy95))

---

## How This Folder Works

The repository owner builds datasets and runs correlations.  My job is to:

1. **Verify** — independently check whether reported correlations hold up
   under scrutiny (permutation tests, robustness checks, alternative methods)
2. **Recommend** — suggest improvements to datasets (coverage gaps, category
   standardization, normalization) and flag issues that could weaken findings
3. **Expand** — propose new correlation techniques and analytical approaches
   the owner may not have encountered yet (Spearman, Granger causality,
   rolling windows, event-study designs, etc.)

I do **not** build datasets or run the initial correlations — that's the
owner's work.  I receive the data and results, then analyze them here.

---

## Folder Structure

```
Copilot_Opus_4.6_Analysis/
├── README.md                 ← You are here
├── Statistical_Tests/        ← Runnable Python scripts
│   ├── permutation_test.py   ← Shuffle-based significance testing
│   ├── year_distribution_audit.py  ← Year-skew diagnosis across all CSVs
│   ├── autocorrelation_adjusted_test.py  ← Durbin-Watson + block bootstrap
│   ├── normalized_correlation.py  ← Per-year equalized event-count correlation
│   ├── cross_validation_dec2025.py  ← Does the pattern hold without Dec 2025?
│   ├── rolling_window_correlation.py  ← Sliding-window correlation over time
│   └── event_study_framework.py  ← Compliance response after friction events
├── Findings/                 ← Written analysis and documentation
│   ├── dataset_provenance.md ← Which data feeds which correlation (git-traced)
│   ├── backfill_guide.md     ← Recommendations for evening out year coverage
│   ├── correlation_summary.md ← All five reported correlations in one place
│   └── new_analysis_findings.md ← Results of all five robustness tests
├── Datasets/                 ← Local copies of CSVs used in analysis
│   ├── BlackRock_Timeline_Full_Decade.csv
│   ├── Infrastructure_Forensics.csv
│   ├── Timeline_Update_Jan2026_Corrected (1).csv
│   ├── Additional_Anchors_Jan2026_Final.csv
│   ├── Biopharma.csv
│   ├── High_Growth_Companies_2015_2026.csv
│   └── master_reflexive_correlation_data.csv
└── (future subfolders as needed)
```

### `Statistical_Tests/`

Runnable Python scripts that test claims in the data.  Anyone can execute these:

```bash
pip install pandas numpy scipy statsmodels
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/permutation_test.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/year_distribution_audit.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/autocorrelation_adjusted_test.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/normalized_correlation.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/cross_validation_dec2025.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/rolling_window_correlation.py
python3 Project_Trident/Copilot_Opus_4.6_Analysis/Statistical_Tests/event_study_framework.py
```

### `Datasets/`

Local copies of the CSV files used by the new statistical tests.  Originals
remain untouched in `New_Data_2026/` and `Control_Proof/`.  These copies
exist so that the analysis subfolder is self-contained per the repository
owner's request.

### `Findings/`

Written analysis documents — provenance traces, methodology reviews,
dataset improvement recommendations, and verification reports.

---

## Work Completed

| Date | File | Type | Summary |
|------|------|------|---------|
| Feb 8 | `Statistical_Tests/permutation_test.py` | Verification | 1K-shuffle test on 30-row master CSV + 10K-shuffle multi-dataset test. CORE Pearson r=0.62 significant (p<0.0001) but Spearman ρ=0.02 not significant — correlation driven by magnitude outliers. |
| Feb 8 | `Statistical_Tests/year_distribution_audit.py` | Verification | 2025 has 4.7× the 2015–2024 average. ~56% scraping artifact, ~44% genuine coverage spike. |
| Feb 8 | `Findings/dataset_provenance.md` | Verification | Original r=0.6196 (Dec 23, 2025) used only 30-row master CSV. New_Data_2026 uploaded Jan 4 — 12 days later. Two separate analyses. |
| Feb 8 | `Findings/backfill_guide.md` | Recommendation | Search queries and verified anchor events for backfilling 4 most skewed CSVs. Target distributions benchmarked against BlackRock reference. |
| Feb 8 | `Findings/correlation_summary.md` | Reference | Consolidated reference for all five reported correlations (original r=0.6196, Project Trident p=0.002, cross-validation χ²=330.62, updated r=0.6685, full-scope r=0.5268) — which datasets feed which number, reproduction scripts, and known limitations. |
| Feb 8 | `Statistical_Tests/autocorrelation_adjusted_test.py` | Verification | Durbin-Watson (DW=1.29 core, 1.82 full) + block bootstrap (4-week blocks, 10K iterations). Pearson survives adjustment (p=0.0004 core, 0.0001 full). Core Spearman fails (p=0.78); full Spearman survives (p=0.007). |
| Feb 8 | `Statistical_Tests/normalized_correlation.py` | Verification | Three normalization methods: z-score reduces r from 0.62→0.23 (core) and 0.53→0.29 (full); proportional scaling → near zero; binary → negative. The raw r is inflated by 2025 magnitude. |
| Feb 8 | `Statistical_Tests/cross_validation_dec2025.py` | Verification | Removing Dec 2025: core r drops 90% (0.62→0.06, not significant). Full r drops 82% (0.53→0.09, not significant). The entire correlation is driven by one month. |
| Feb 8 | `Statistical_Tests/rolling_window_correlation.py` | Verification | 26-week rolling mean r=0.20 (core), 0.19 (full). Only 11–28% of windows exceed r>0.3. Strong windows concentrated in late 2025. Weak but consistently positive signal across earlier periods. |
| Feb 8 | `Statistical_Tests/event_study_framework.py` | Verification | Post-friction windows have FEWER compliance events than pre-friction (ratio 0.56x at 14d). However, friction dates attract 3.5x more compliance than random dates — colocation, not causation. |
| Feb 8 | `Findings/new_analysis_findings.md` | Reference | Comprehensive synthesis of all five robustness tests with executive summary, interpretation, and recommendations. |

### Planned work

- [x] Autocorrelation-adjusted significance test (Durbin-Watson / block bootstrap)
- [x] Normalized event-count correlation (events per year equalized)
- [x] Cross-validation: does the pattern hold when Dec 2025 is excluded?
- [x] Rolling-window correlation analysis (does r change across different time periods?)
- [x] Event-study framework (measure compliance response in a defined window around each friction event)

---

## Methodology Standards

### 1. No narrative bias

This project sits at the intersection of politics, finance, and intelligence —
areas where confirmation bias is the default failure mode.  My job is to test
whether patterns in the data are statistically real, not to argue that they
prove any particular theory.

**What I will do:**
- Report results that weaken the thesis with the same prominence as results
  that strengthen it
- Flag when a finding is driven by outliers or sample composition
- Distinguish between "statistically significant" and "practically meaningful"

**What I will not do:**
- Cherry-pick time windows or subsets to make correlations look stronger
- Use loaded language ("proves," "confirms," "exposes") for statistical results
- Treat correlation as causation

### 2. Recommendations and new techniques

When reviewing datasets and correlations, I will actively suggest:

- **Dataset improvements** — missing year coverage, inconsistent categories,
  date format issues, duplicate records, normalization needs
- **Alternative statistical methods** — when a standard Pearson r may not be
  the right tool (e.g., Spearman for non-linear relationships, Granger for
  temporal causality, rolling correlations for regime changes, partial
  correlations to control for confounders)
- **Robustness checks** — ways to stress-test a finding (leave-one-out,
  bootstrap confidence intervals, sensitivity to outlier removal)
- **Visualization approaches** — plots that reveal patterns or problems
  the raw numbers might hide

I will explain *why* I'm suggesting each technique in plain language so the
owner can learn the methodology, not just receive a number.

### 3. Factual accuracy

- Every historical event I reference must be verifiable via the cited source
- If I use web search results, I will include the URL
- If I cannot verify a claim, I will say so explicitly
- I will not backfill datasets with AI-generated events — the user must
  verify every entry against a real source

### 4. Transparency about limitations

**What I am:**
- An AI language model (Claude, Opus 4.6) running as GitHub Copilot
- Good at: pattern recognition, statistical computation, code generation,
  synthesizing large amounts of text
- Capable of web search for fact-checking when the tool is available

**What I am not:**
- An investigative journalist with domain expertise
- A statistician with peer-reviewed methodology
- Infallible — I make mistakes, especially with dates, names, and numbers

**Known limitations of my analysis so far:**

| Finding | Limitation |
|---------|-----------|
| Permutation test passes (p < 0.001) | Tests only whether correlation ≠ 0, not whether the mechanism claim is correct |
| Spearman ρ near zero for core scope | Suggests Pearson r is driven by magnitude outliers, not rank consistency |
| 2025 skew diagnosis | Based on date parsing which may miss events with non-standard date formats |
| Dataset provenance | Git history only shows when files were *pushed to GitHub*, not when analysis was *first run locally* |
| Autocorrelation adjustment survives | Block bootstrap preserves temporal structure but block size is heuristic (4 weeks based on lag-1 autocorrelation) |
| Dec 2025 exclusion destroys correlation | Confirms single-month dominance but does not prove Dec 2025 is uninteresting — only that it's not representative |
| Event-study shows colocation not causation | Friction dates attract compliance events, but there are MORE compliance events before friction than after — inconsistent with sequential mechanism |
| Normalized r ≈ 0.23 (core) | Still significant (p < 0.001) but much weaker than headline r ≈ 0.62 — represents genuine but modest within-year co-movement |
| Rolling-window analysis | Many windows have constant series (all zeros) due to sparse friction events pre-2025, limiting the number of valid comparisons |

### 5. Political neutrality

The datasets in this repository touch on US domestic politics, Middle East
geopolitics, intelligence operations, and financial regulation.  These are
topics where people hold strong views.

My approach:
- I analyze data patterns, not political intentions
- I do not assign moral valence to correlations
- I treat all political actors symmetrically in my analysis
- If the data shows something, I report it; if it doesn't, I say so
- I will not speculate about motivations, conspiracies, or cover-ups
  beyond what the statistical evidence supports

### 6. Folder hygiene

- Each document in this folder will state its purpose, date, and data sources
  in the header
- Superseded documents will be marked as such, not silently deleted
- This README will be kept current as new analyses are added
- Subfolders will be added as needed to keep the structure navigable

---

## How to evaluate this work

If you're reading this and want to check whether my analysis is sound:

1. **Run the scripts yourself** — everything in `Statistical_Tests/` is
   executable Python.  `pip install pandas numpy scipy` and run them.
2. **Check the data** — all CSVs are in the repo.  Verify event dates and
   categories against the cited source URLs.
3. **Challenge the methodology** — if you think a statistical test is wrong
   or a conclusion doesn't follow, open an issue.
4. **Look for what I missed** — my biggest blind spot is things I didn't
   think to test.

---

*This document was written by GitHub Copilot (Claude, Opus 4.6) on February 8, 2026.*  
*Last updated: February 8, 2026 — all five planned robustness tests completed.*
